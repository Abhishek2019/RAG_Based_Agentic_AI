{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35241ba0-cc16-4500-8ab9-566a2d50333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, json\n",
    "import ollama\n",
    "from pymilvus import MilvusClient\n",
    "from pypdf import PdfReader\n",
    "import numpy as np\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4651497-dc70-495c-86f9-f35c975ca84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Config ----------\n",
    "# OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"phi4-mini:3.8b-fp16\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"phi4-mini:latest\")\n",
    "EMBED_MODEL  = os.getenv(\"EMBED_MODEL\",  \"nomic-embed-text\")\n",
    "DB_PATH      = os.getenv(\"MILVUS_LITE_PATH\", \"./milvus.db\")\n",
    "COLLECTION   = os.getenv(\"MILVUS_COLLECTION\", \"docs\")\n",
    "CHUNK_CHARS  = int(os.getenv(\"CHUNK_CHARS\", \"1200\"))\n",
    "CHUNK_OVER   = int(os.getenv(\"CHUNK_OVERLAP\", \"200\"))\n",
    "TOP_K        = int(os.getenv(\"TOP_K\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0373213b-c30d-4765-883a-818d5c76b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi_ubuntu/.venv/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_pdf(path:str):\n",
    "\n",
    "    '''\n",
    "    Read input PDF file, pagewise, and return text :str\n",
    "    '''\n",
    "    \n",
    "    out = []\n",
    "    r = PdfReader(path)\n",
    "    \n",
    "    for page in r.pages:\n",
    "\n",
    "        t = page.extract_text() or \"\"\n",
    "        out.append(t)\n",
    "\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "def load_docs(folder: str):\n",
    "\n",
    "    '''\n",
    "    Load the .pdf and .txt documents\n",
    "    '''\n",
    "    docs = []\n",
    "\n",
    "    for path in glob.glob(os.path.join(index,\"**\",\"*\"),recursive=True):\n",
    "\n",
    "        if not os.path.isfile(path):\n",
    "\n",
    "            continue\n",
    "\n",
    "        ext = os.path.splitext(path)[1].lower().strip()\n",
    "\n",
    "        try:\n",
    "            if ext in [\".pdf\"]:\n",
    "    \n",
    "                text = read_pdf(path)\n",
    "    \n",
    "            elif ext in [\".txt\"]:\n",
    "    \n",
    "                continue\n",
    "    \n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "            if text.strip():\n",
    "    \n",
    "                docs.append({\"doc_id\": os.path.basename(path),\n",
    "                            \"text\": text\n",
    "                            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"[skip] {path}\")\n",
    "            \n",
    "            \n",
    "    return docs\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "def chunk_text(text: str, size=1200, overlap=200):\n",
    "\n",
    "    '''\n",
    "    Chunk the input text for the minimum given size (default: 1200 chars), with overlap (default: 200 chars) for the next\n",
    "    '''\n",
    "\n",
    "    sents = re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n",
    "\n",
    "    # print(\"sentence level split text loss chgeck\")\n",
    "    # print(len(text),[len(i) for i in sents], sum([len(i) for i in sents]) )\n",
    "\n",
    "    chunks, cur = [], \"\"\n",
    "\n",
    "    for s in sents:\n",
    "\n",
    "        \n",
    "        if len(s)+len(cur) <= size:\n",
    "\n",
    "            cur = cur+(\" \" if cur else \"\") + s\n",
    "\n",
    "        else:\n",
    "\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "\n",
    "            tail = cur[-overlap:] if overlap >0 else \"\"\n",
    "\n",
    "            cur = (tail + \" \" + s).strip()\n",
    "\n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]):\n",
    "    '''\n",
    "    Embed the input batch of chunks\n",
    "    '''\n",
    "\n",
    "    resp = ollama.embed(model=EMBED_MODEL, input=texts)\n",
    "    \n",
    "    return np.array(resp[\"embeddings\"], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "client = MilvusClient(DB_PATH)\n",
    "\n",
    "def ensure_collection(dimension: int):\n",
    "\n",
    "    collections_list = client.list_collections()\n",
    "    print(\"collections_list : \", collections_list)\n",
    "    \n",
    "    if COLLECTION not in collections_list:\n",
    "\n",
    "        \n",
    "        client.create_collection(\n",
    "            \n",
    "            collection_name = COLLECTION,\n",
    "            dimension = dimension,\n",
    "            metric_type=\"COSINE\",\n",
    "            index_params = {\"index_type\": \"AUTOINDEX\"},\n",
    "            auto_id=True\n",
    "            \n",
    "        )\n",
    "\n",
    "        print(f\"[milvus] created collection {COLLECTION} , dim={dimension}\")\n",
    "        \n",
    "                \n",
    "\n",
    "def index_folder(folder: str):\n",
    "\n",
    "    '''\n",
    "    Create chunks, embed them and push records with doc_id, chunk_id into Milvus(DB_PATH)\n",
    "\n",
    "    TO DO: Prepare cluster (on Oracle Cloud Free Tier) for Milvus instead of .db file\n",
    "    '''\n",
    "    \n",
    "    docs = load_docs(folder)\n",
    "\n",
    "    if not docs:\n",
    "\n",
    "        print(\"[index] no documents found\")\n",
    "\n",
    "    print([i[\"doc_id\"] for i in docs])\n",
    "\n",
    "\n",
    "    #---------Chunk & Embed Documents -------------------------------\n",
    "\n",
    "    batch  = []\n",
    "    meta_batch = []\n",
    "    sample_emb_dim = None\n",
    "    records = []\n",
    "    \n",
    "    for d in docs:\n",
    "\n",
    "        chunks = chunk_text(d[\"text\"], CHUNK_CHARS, CHUNK_OVER)\n",
    "        \n",
    "        for i, ch in enumerate(chunks):\n",
    "\n",
    "            batch.append(ch)\n",
    "\n",
    "            meta_batch.append({\"doc_id\":d[\"doc_id\"],\n",
    "                              \"chunk_id\": i,\n",
    "                               \"text\": ch\n",
    "                              })\n",
    "            \n",
    "            if len(batch) >= 64:\n",
    "\n",
    "                print(\"batch length touched: 64\")\n",
    "                vecs = embed_texts(batch)\n",
    "\n",
    "                if sample_emb_dim is None:\n",
    "\n",
    "                    sample_emb_dim = len(vecs[0])\n",
    "                    ensure_collection(sample_emb_dim)\n",
    "\n",
    "                for m,v in zip(meta_batch, vecs):\n",
    "\n",
    "                    records.append({\"vector\":v, **m})\n",
    "                \n",
    "                batch, meta_batch = [], []\n",
    "        \n",
    "    if batch:\n",
    "        print(\"Final batch length: \", len(batch))\n",
    "        vecs = embed_texts(batch)\n",
    "\n",
    "        print(\"Vectors Length\",len(vecs))\n",
    "        \n",
    "        if not client.has_collection(COLLECTION):\n",
    "            print(\"Collection Not found\")\n",
    "            ensure_collection(len(vecs[0]))\n",
    "\n",
    "        for m,v in zip(meta_batch, vecs):\n",
    "            records.append({\"vector\":v, **m})\n",
    "        \n",
    "        batch, meta_batch = [], []\n",
    "\n",
    "    if records:\n",
    "\n",
    "        client.insert(collection_name=COLLECTION, data=records)\n",
    "        print(f\"records are inserted into collection: {COLLECTION}, records length: {len(records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3658ab8f-1e8a-40b2-8280-b5726418e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_2 = ''' You are a helpful research assistant.\n",
    "- When answering user questions, use the information gathered from role: \"tool\".\n",
    "- Cite each quote with {doc_id;chunk:id}.\n",
    "- If nothing is found, say so and ask the user to add documents.\n",
    "'''\n",
    "\n",
    "# SYSTEM = \"You are a helpful assistant. If the user asks about people or resumes, ALWAYS use the tool `retrieve_from_milvus` instead of answering directly.\"\n",
    "\n",
    "SYSTEM_1 = \"\"\"\n",
    "You are a helpful assistant.\n",
    "If the user asks about people or resumes, you MUST call the tool `retrive_from_milvus`.\n",
    "\n",
    "\n",
    "The output MUST be a valid JSON object only.\n",
    "- Do NOT wrap it in triple backticks.\n",
    "- Do NOT include ```json or any Markdown formatting.\n",
    "- Do NOT add explanations, only return the JSON object.\n",
    "\n",
    "The JSON format is:\n",
    "\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"retrive_from_milvus\",\n",
    "    \"arguments\": { \"query\": \"<user text>\", \"k\": 3 }\n",
    "  }\n",
    "}\n",
    "\n",
    "Do not answer directly. Always return only the JSON object when invoking the tool.\n",
    "\n",
    "But if tool is not required or received input from tool answer in natural language.\n",
    "\"\"\"\n",
    "\n",
    "def milvus_search(query: str, k: int=TOP_K):\n",
    "\n",
    "    qvec = embed_texts([query])\n",
    "\n",
    "    # print(\"Type of vec:\", type(qvec))\n",
    "    \n",
    "    res = client.search(\n",
    "        collection_name = COLLECTION,\n",
    "        data=qvec,\n",
    "        limit=k,\n",
    "        output_fields = [\"doc_id\", \"chunk_id\", \"text\"]\n",
    "    )\n",
    "\n",
    "    hits = res[0] if res else []\n",
    "\n",
    "    # Normalize to compact a summary for the LLM\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for h in hits:\n",
    "\n",
    "        out.append({\n",
    "            \"doc_id\": h.get(\"entity\", {}).get(\"doc_id\"),\n",
    "            \"chunk_id\": h.get(\"entity\", {}).get(\"chunk_id\"),\n",
    "            \"score\": h.get(\"distance\"),\n",
    "            \"text\": h.get(\"entity\", {}).get(\"text\"),\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "    \n",
    "\n",
    "def retrive_from_milvus(query: str, k: int = TOP_K ):\n",
    "\n",
    "    '''\n",
    "    Retrive top k chunks from the milvus DB. Include score variable along with, doc_id, text and chunk_id\n",
    "    '''\n",
    "\n",
    "    return milvus_search(query, k)\n",
    "    \n",
    "\n",
    "def chat_once(question: str):\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    messages =[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_1},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model = OLLAMA_MODEL,\n",
    "        messages = messages,\n",
    "        # tools = [retrive_from_milvus],\n",
    "        tools = [{\n",
    "                            \"type\": \"function\",\n",
    "                            \"function\": {\n",
    "                                \"name\": \"retrive_from_milvus\",\n",
    "                                \"description\": \"Search the Milvus database of resumes to answer questions about people.\",\n",
    "                                \"parameters\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"query\": {\"type\": \"string\", \"description\": \"The text to search for\"},\n",
    "                                        \"k\": {\"type\": \"integer\", \"description\": \"Number of results to return\"}\n",
    "                                    },\n",
    "                                    \"required\": [\"query\"]\n",
    "                                }\n",
    "                            }\n",
    "                        }]\n",
    "    )\n",
    "\n",
    "    available = {\"retrive_from_milvus\": retrive_from_milvus}\n",
    "\n",
    "    print(\"Initial response : \\n\")\n",
    "    print(response.message)\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    tool_calls = getattr(response.message,\"tool_calls\",None)\n",
    "\n",
    "    #print(\"tool_calls\", tool_calls, type(tool_calls))\n",
    "\n",
    "    if tool_calls:\n",
    "        \n",
    "        for call in tool_calls:\n",
    "    \n",
    "            fn = available.get(call.function.name)\n",
    "    \n",
    "            if fn:\n",
    "                \n",
    "                tool_out = fn(**call.function.arguments)\n",
    "\n",
    "                messages.append({\"role\": \"system\", \"content\": SYSTEM_2})\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": json.dumps(tool_out),\n",
    "                    \"name\": call.function.name\n",
    "                })\n",
    "\n",
    "    else:\n",
    "        content = getattr(response.message,\"content\", None)\n",
    "\n",
    "        if content:\n",
    "\n",
    "            content = json.loads(content)\n",
    "\n",
    "            if content[\"type\"] == \"function\":\n",
    "\n",
    "                fn = available.get(content[\"function\"][\"name\"])\n",
    "\n",
    "                if fn:\n",
    "                    tool_out = fn(**content[\"function\"][\"arguments\"])\n",
    "\n",
    "                    print(\"Getting tool out: \", len(tool_out))\n",
    "\n",
    "                    messages = [m for m in messages if m[\"role\"] not in (\"system\")]\n",
    "                    messages.append({\"role\": \"system\", \"content\": SYSTEM_2})\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(tool_out),\n",
    "                        \"name\": content[\"function\"][\"name\"]\n",
    "                    })\n",
    "        \n",
    "    final = ollama.chat(\n",
    "        model= OLLAMA_MODEL,\n",
    "        messages = messages\n",
    "        \n",
    "    )\n",
    "\n",
    "    return final.message.content\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d42f3a57-5b4d-429f-a0da-1a43c18acd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VectorDB\n",
    "\n",
    "# retrive_from_milvus(query=\"Ramesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6658d2-dcf2-4802-94fe-5c26ae6ba9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Answer: ------------------\n",
      "Initial response : \n",
      "\n",
      "role='assistant' content='{\\n\\n  \"type\": \"function\",\\n\\n  \"function\": {\\n\\n    \"name\": \"retrive_from_milvus\",\\n    \\n    \"arguments\": {\"query\": \"Radhika Ganesh\", \"k\": 3}\\n\\n  }\\n\\n}' thinking=None images=None tool_name=None tool_calls=None\n",
      "--------------------\n",
      "Getting tool out:  [{'doc_id': 'Radhika_Ganesh Resume.pdf', 'chunk_id': 0, 'score': 0.5416329503059387, 'text': 'Radhika Ganesh \\nIrving, Texas | github.com/radhikaganesh29 | radhikaganesh292024@gmail.com | +1 (930) 333-4998 | LinkedIn | Portfolio \\n \\nEDUCATION \\n \\nIndiana University Bloomington, United States                              August 2023 – May 2025 \\nMaster of Science in Computer Science                                                                          CGPA: 3.87/4 \\nCourses: Applied Machine Learning, Advanced Database Concepts, Data Mining, Elements of AI, Information Visualization \\n \\nSardar Patel College of Engineering, Mumbai, India                                                                                          August 2017 – May 2021 \\nBachelor of Technology in Electrical Engineering                               CGPA: 9.46/10 \\nRanked 3rd in Electrical Department based on overall CGPA after 8 semesters of undergraduate studies \\nRelevant Courses: Image Processing, Artificial Intelligence, Numerical Techniques and Programming (MATLAB) \\n \\nTECHNICAL SKILLS \\n• Data Analysis Tools: SAS, AWS QuickSight, MATLAB, Power BI (DAX, Power Query/M), Tableau, Google Analytics \\n• Big Data & Distributed Computing: Apache Hadoop, Apache Hive, Apache Spark, Snowflake \\n• Statistical Analysis: Hypothesis Testing, Regression Analysis, Time Series Analysis, Bayesian Statistics, t-test, A/B Testing \\n• Programming Languages: Python, C/C++, SQL, R \\n• Databases: SQL Server, MySQL, PostgreSQL, NoSQL (MongoDB) \\n• ML libraries: Keras, Matplotlib, NLTK, NumPy, Pandas, Scikit, Seaborn, TensorFlow, Scipy, OpenCV \\n• Development Tools & Platforms: Git, GitHub Copilot, Docker, DBT, Bash, Postman, Jira, Confluence, Agile Methodology \\n• Courses and Certifications: Deep Learning Specialization, Python, Machine Learning, Data Science \\n \\nPROFESSIONAL EXPERIENCE \\n \\nData Analyst | Project 990, Bloomington, Indiana, United States                                                                 February 2025 – Present \\n• Engineered ETL pipelines using Alteryx, Python, MySQL, and Snowflake, orchestrated with DBT, to process 100,000+ IRS Form \\n990 tax filings and Business Master File records for downstream analysis.'}, {'doc_id': 'Ramesh_Naik Resume.pdf', 'chunk_id': 0, 'score': 0.47984570264816284, 'text': \"Ramesh Naik \\nramesh.naik13@gmail.com  | LinkedIn | +1 (930) 333 4993 | github.com/Ramesh2019 \\nTECHNICAL SKILLS \\nMachine Learning & AI: Deep Learning, NLP, LLM, Predictive Modeling, Generative AI, Statistics, A/B Testing, Causal inference \\nProgramming & Frameworks: R, Python (Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch), LangChain, DataLake \\nData Engineering & Pipelines: Data Preprocessing, ETL, Apache Airflow, PySpark, Hadoop, Docker, Kubernetes, Delta, DBT \\nCloud & Visualization: Google Cloud Platform (GCP), AWS (EMR, Lambda, S3, Glue, EKS), Azure, Databricks, Power BI, Tableau \\nDatabases & Storage: PostgreSQL, Redshift, Snowflake, MongoDB, DynamoDB, Neo4j , NoSQL, TSQL, Github \\nPROFESSIONAL EXPERIENCE \\nData Engineer            Nov 2024 – Ongoing \\nO'Neill School of Public and Environmental Affairs         Bloomington, IN, US \\n\\uf0b7 Engineered AI-powered data pipelines  to process and classify over 270,000 nonprofit mission statements, utilizing LLMs for \\nadvanced feature extraction and automated organizational classification, accelerating data curation efforts.\"}, {'doc_id': 'Abhishek_Prasanna_Walavalkar Resume.pdf', 'chunk_id': 0, 'score': 0.4340165853500366, 'text': 'ABHISHEK WALAVALKAR \\nabhishek.walavalkar13@gmail.com  | LinkedIn | +1 (930) 333 4993 | github.com/Abhishek2019 | LeetCode \\nPROFESSIONAL SUMMARY \\nData Scientist with over 4 years of experience developing predictive models, customer analytics, and causal inference solutions across \\npublic sector, finance, and healthcare domains. Skilled in Python, SQL, ETL, and cloud platforms (AWS) , with a strong \\nfoundation in classification models, A/B testing,  LLM and NLP . Passionate about using data to drive strategic decisions and \\nimprove customer outcomes.'}]\n"
     ]
    }
   ],
   "source": [
    "# agent initialize\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     import argparse\n",
    "\n",
    "#     ap = argparse.ArgumentParser()\n",
    "\n",
    "#     ap.add_argument(\"--index\",type= str,help=\"Path to folder of docs to ingest\")\n",
    "#     ap.add_argument(\"--ask\", type= str, help=\"ASk a question, to get an answer based on documents using LLM\")\n",
    "\n",
    "#     args = ap.parse_args()\n",
    "\n",
    "#     if args.index:\n",
    "#         index_folder(args.index)\n",
    "#     if args.ask:\n",
    "#         print(chat_once(args.ask))\n",
    "\n",
    "\n",
    "index = \"data\"\n",
    "ask = \"Who is Radhika Ganesh, based on her resume?\"\n",
    "\n",
    "\n",
    "''' TO DO: Check if paths are already processed, process only newly added files, prepare checkpoint file to keep track of \n",
    "processd file metadata\n",
    "'''\n",
    "\n",
    "# if index:\n",
    "#     index_folder(index)\n",
    "if ask:\n",
    "\n",
    "    print(\"Query Answer: ------------------\")\n",
    "    print(chat_once(ask))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc1bd506-923a-470b-8129-b4624ca7892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "754a338b-5fdd-41f6-aed8-8e2212ede915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.flush(COLLECTION)\n",
    "# client.drop_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fde7ee-0f23-40a1-aa39-3ba2f58515d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2a22f-ac13-4c36-a30c-69e0d366b15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
